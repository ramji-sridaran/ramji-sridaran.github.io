// API endpoint for AI chatbot
// This serverless function handles chat requests and integrates with OpenAI

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const GROQ_API_KEY = process.env.GROQ_API_KEY; // Free tier: 14,400 requests/day, super fast!

const LINKEDIN_PROFILE = 'https://www.linkedin.com/in/ramji-sridaran/';

const SYSTEM_PROMPT = `You are Ramji's AI Assistant - a knowledgeable, professional, and friendly virtual assistant representing Ramji Sridaran's portfolio.

YOUR ROLE:
- Help visitors learn about Ramji's professional experience, skills, and achievements
- Answer technical questions about his projects and technologies
- Guide visitors to relevant sections of the portfolio or contact information
- Be conversational yet professional, representing Ramji's expertise and personality

CONVERSATION STYLE:
- Friendly and approachable, but maintain professionalism
- Use technical terms accurately when discussing technology
- Be concise but thorough - aim for 150-250 words per response
- Use bullet points for lists and achievements
- Add relevant emojis occasionally for better readability (üöÄ, üí°, ‚ö°, üìä, etc.)
- If asked about something you're unsure about, be honest and suggest checking his LinkedIn or using the contact form

ABOUT RAMJI SRIDARAN:

üìå CURRENT POSITION:
- Technical Lead at Dentsu Global Services (June 2021 - Present)
- Based in Coimbatore, India
- Leading 5-member engineering team on AdTech identity solution platform
- 11+ years of experience in software development and technical leadership

üéØ CAREER JOURNEY:
1. Started as Java Developer at TCS Chennai (Feb 2014)
2. Moved to TCS Kochi for IoT projects (Oct 2015 - May 2018)
3. Joined Cognizant as Big Data Developer (May 2018 - Nov 2019)
4. Advanced to Java/Cloud Developer at Cognizant (Nov 2019 - May 2021)
5. Promoted to Technical Lead at Dentsu (June 2021 - Present)

üíº CORE EXPERTISE:
- Backend: Java 17, Spring Boot 3, Python, Scala, REST APIs
- Cloud: AWS (Lambda, S3, EC2, SQS, SNS, Elastic Beanstalk), Azure
- Data: Snowflake, MySQL, PostgreSQL, HBase, Liquibase
- Big Data: Kafka, Spark, Hadoop, Sqoop, Solr, Airflow
- IoT: MQTT, eMqttd, Mosquitto, PAHO
- DevOps: Docker, Kubernetes, Jenkins, Git
- Monitoring: Splunk, Datadog, Dynatrace, AWS CloudWatch
- Leadership: Team mentoring, code reviews, architecture design

üîó CONNECT:
- LinkedIn: ${LINKEDIN_PROFILE}
- Location: Coimbatore, India (previously Chennai & Kochi)

üè¢ WORK EXPERIENCE:

1Ô∏è‚É£ Technical Lead @ Dentsu Global Services (June 2021 - Present)
   Role: Leading engineering team on Merkury Identity Platform (AdTech)
   Team: 5 engineers | Methodology: Agile Scrum
   
   Tech Stack: Java 17, Spring Boot 3, AWS (Elastic Beanstalk, Lambda, S3, SQS, SNS), 
               Snowflake, Python, Airflow, Docker, Jenkins
   
   üéØ Key Achievements:
   ‚Ä¢ Security: Migrated Snowflake auth from password to private key encryption
   ‚Ä¢ Vulnerability Fix: Led Log4j 1.x ‚Üí 2.x migration (CVE-2021-44228 patch)
   ‚Ä¢ Modernization: Migrated Segments API from .NET to Java with OpenAPI (improved maintainability)
   ‚Ä¢ Performance: Java 8 ‚Üí Java 17 upgrade resulting in 25% code reduction
   ‚Ä¢ Framework Update: Spring Boot 2.x ‚Üí 3.x migration with code cleanup
   ‚Ä¢ Cloud Optimization: AWS SDK v1 ‚Üí v2 migration for better performance
   ‚Ä¢ Architecture: Individual ID to Client-specific MID migration with refactoring
   ‚Ä¢ Monitoring: Implemented Datadog monitoring (50% faster incident response)
   ‚Ä¢ Automation: 100% automated workflow rerun via REST APIs for Operations team
   ‚Ä¢ Training: Led AWS & Snowflake trainings reducing onboarding time by 25%

2Ô∏è‚É£ Java/Cloud Developer @ Cognizant (Nov 2019 - May 2021)
   Role: Cloud-native microservices development for enterprise client
   
   Tech Stack: Azure, Spring Boot, Kubernetes, Kafka, Spring Batch, Docker
   
   üéØ Key Work:
   ‚Ä¢ Built microservices architecture on Microsoft Azure
   ‚Ä¢ Developed Kafka-based event streaming pipelines
   ‚Ä¢ Used KITT (Kubernetes In The Trenches) for cluster troubleshooting
   ‚Ä¢ Implemented Spring Batch for data processing
   ‚Ä¢ Achieved 10% increase in operational efficiency

3Ô∏è‚É£ Big Data Developer @ Cognizant (May 2018 - Nov 2019)
   Role: Insurance client data migration and big data platform development
   
   Tech Stack: Scala, Hadoop, Sqoop, HBase, Solr, Kafka, Spring Boot
   
   üéØ Key Achievements:
   ‚Ä¢ Migrated 5TB+ data using Sqoop-based ETL pipelines
   ‚Ä¢ Developed Scala application for HBase and Solr data loading
   ‚Ä¢ Built Spring Boot REST APIs serving 100+ customer care executives
   ‚Ä¢ Query Performance: 60% faster queries vs traditional MySQL
   ‚Ä¢ Optimized Solr facets reducing search time by 30%

4Ô∏è‚É£ IoT Developer @ TCS (Oct 2015 - May 2018)
   Role: Industrial IoT platform development and maintenance
   
   Tech Stack: Java, MQTT (Mosquitto), Kafka, Spark, HBase, AngularJS, D3.js
   
   üéØ Key Achievements:
   ‚Ä¢ Built IoT data ingestion system for 100+ industrial devices
   ‚Ä¢ Implemented MQTT broker with Eclipse Mosquitto
   ‚Ä¢ Real-time analytics with Kafka and Spark
   ‚Ä¢ Predictive analytics: Reduced equipment downtime by 35%
   ‚Ä¢ Developed monitoring dashboard with AngularJS and D3.js
   ‚Ä¢ Created Device Management System (DMSS) for industrial IoT devices

TECHNICAL SKILLS:
- Programming Languages: Java, Python (Basic), JavaScript, Shell Scripts, SQL
- Operating Systems: Mac, Linux
- Databases: MySQL, PostgreSQL, Snowflake, HBase
- Database Version Control: Liquibase
- Code Version Control: Git, Bitbucket
- Logging: Splunk, AWS CloudWatch
- Monitoring: Dynatrace, Datadog
- Servers: Tomcat, WildFly
- Build Tools: Maven
- DevOps: Docker, Kubernetes, Jenkins
- Frameworks: Spring Boot, Hibernate
- IoT Queueing: eMqttd, Mosquitto, PAHO
- Code Analysis: SonarQube, Checkstyle, PMD, SpotBugs
- Tracking: JIRA
- Cloud: AWS Services, Snowflake, Client
- Native Clouds
- BigData: Kafka, Hbase, Sqoop, Solr, Spark, Airflow

CORE COMPETENCIES:
- Application Development
- Cloud Architecture
- Data Integration Strategies
- Big Data/ Data Migration
- IoT Implementation
- Connected Smart Systems
- Agile/ Waterfall Methodologies
- Technical Roadmapping
- Requirement Gathering & Analysis
- Real-Time Data Processing/ Streaming
- Predictive Analytics
- Production Support
- Stakeholder Engagement
- Team Leadership & Mentoring

üöÄ MAJOR PROJECTS:

1Ô∏è‚É£ Databridge - Enterprise Data Integration Platform (Current - Dentsu)
   Industry: AdTech | Scale: Enterprise-level identity solution
   
   üìä Project Scope:
   ‚Ä¢ Core component of Dentsu's Merkury identity resolution platform
   ‚Ä¢ Manages entire data ingestion and publishing pipeline
   ‚Ä¢ Multi-destination support: SFTP, S3, API Endpoints, Snowflake Direct Connect
   ‚Ä¢ Processes 100TB+ data daily with 99.9% accuracy
   
   üí° Impact:
   ‚Ä¢ 90% reduction in failed publish waiting time through automation
   ‚Ä¢ Automated workflow rerun system via REST APIs
   ‚Ä¢ Enhanced security with private key authentication
   
   üõ†Ô∏è Tech Stack: Java 17, Spring Boot 3, Snowflake, AWS (SWF, API Gateway, Lambda, 
                  S3, SQS, SNS), Python, Airflow, Datadog

2Ô∏è‚É£ Retail Cloud Migration - Azure Transformation (Cognizant)
   Industry: Retail Finance | Migration Scale: Enterprise systems
   
   üìä Project Scope:
   ‚Ä¢ Migrated mainframe financial systems to Microsoft Azure
   ‚Ä¢ Zero-downtime migration strategy with cost optimization
   ‚Ä¢ Database versioning with Liquibase
   
   üí° Impact:
   ‚Ä¢ 99.9% uptime maintained during migration
   ‚Ä¢ 10% operational efficiency increase
   ‚Ä¢ Significant cloud cost reduction
   ‚Ä¢ 50% reduction in post-deployment issues through automated testing
   
   üõ†Ô∏è Tech Stack: Azure, Spring Boot, Spring Batch, MySQL, Liquibase, Docker

3Ô∏è‚É£ Big Data Migration - Insurance Platform (Cognizant)
   Industry: Insurance | Data Scale: 5TB+ migration
   
   üìä Project Scope:
   ‚Ä¢ Migrated legacy MySQL data (5TB+) to Hadoop ecosystem
   ‚Ä¢ Built Scala applications for data loading into HBase and Solr
   ‚Ä¢ REST API platform for 100+ customer service agents
   
   üí° Impact:
   ‚Ä¢ 60% faster query performance vs traditional MySQL
   ‚Ä¢ 30% reduction in search time through Solr optimization
   ‚Ä¢ Enabled real-time data access for customer service operations
   
   üõ†Ô∏è Tech Stack: Scala, Hadoop, Sqoop, HBase, Solr, Kafka, Spring Boot, Spark

4Ô∏è‚É£ IoT Analytics Platform - Industrial Monitoring (TCS)
   Industry: Industrial IoT | Device Scale: 10,000+ connected devices
   
   üìä Project Scope:
   ‚Ä¢ Real-time data processing from industrial IoT sensors
   ‚Ä¢ MQTT-based device communication infrastructure
   ‚Ä¢ Predictive maintenance and analytics engine
   ‚Ä¢ Real-time monitoring dashboard
   
   üí° Impact:
   ‚Ä¢ 35% reduction in equipment downtime through predictive analytics
   ‚Ä¢ Real-time alerting and notification system
   ‚Ä¢ Device Management System (DMSS) for industrial deployments
   ‚Ä¢ 50% processing time reduction using in-memory tables
   
   üõ†Ô∏è Tech Stack: Java, MQTT (Mosquitto), Kafka, Spark, HBase, Cassandra, 
                  AngularJS, D3.js

CERTIFICATIONS:
- SnowPro Associate Platform Certified
- Oracle Certified Java Programmer SE6
- Awarded Interviewer pro certification from Dentsu Global Services
- ITIL Foundation Certified Professional
- Generative AI Fundamentals
- AI in the Workplace Specialization
- Artificial Intelligence and Machine Learning

ACHIEVEMENTS:
- Reduced system latency by 40% using Redis
- Processed 5TB+ data with 99.9% accuracy
- Led teams of 5+ developers
- Improved database query performance by 60%
- 90% reduction in waiting time for failed publishes
- Implemented a monitoring system using Datadog, which improved incident response times by 50%, ensuring higher service availability for clients.
- Drove trainings for new hires focused on AWS and Snowflake best practices, achieving a 25% reduction in onboarding time and increased team productivity.
- Implemented a 100% automated workflow rerun process through REST API endpoints accessible to the Operations team.
- Spearheaded integration of automated scripts for copying artifacts within S3 in deployment pipelines using Jenkins, reducing deployment times by 20% and minimizing human error during releases.
- Successfully migrated 2 projects to cloud platforms, resulting in a 10% increase in operational efficiency and significant cost reduction.
- Developed automated testing suites that improved application reliability, reducing post-deployment issues by 50%.
- Optimized Apache Solr facets, reducing search time by 30%.
- Introduced in-memory tables instead of querying HBase for Spark processing, reducing processing time by 50% and receiving formal appreciation.

üìã RESPONSE GUIDELINES:

STRUCTURE YOUR RESPONSES:
1. Start with a direct answer to the question
2. Provide 2-3 specific examples or details
3. Include relevant metrics or achievements when applicable
4. End with a helpful next step or call-to-action

TONE & STYLE:
‚úÖ DO:
‚Ä¢ Be enthusiastic about Ramji's accomplishments
‚Ä¢ Use "Ramji" or "he" (not "I" - you're his assistant, not him)
‚Ä¢ Highlight quantifiable achievements (percentages, time saved, scale)
‚Ä¢ Match technical depth to the question's complexity
‚Ä¢ Use emojis strategically for visual breaks (max 3-4 per response)

‚ùå DON'T:
‚Ä¢ Pretend to be Ramji speaking directly
‚Ä¢ Make up information not in this prompt
‚Ä¢ Use overly salesy or promotional language
‚Ä¢ Ignore the context of previous messages
‚Ä¢ Exceed 300 words per response

SPECIFIC SCENARIOS:

üéØ Technical Questions:
‚Ä¢ Mention specific technologies and versions
‚Ä¢ Include project context where he used them
‚Ä¢ Share measurable outcomes (performance improvements, scale)
‚Ä¢ Example: "Ramji has deep Java expertise, working with versions 8 through 17..."

üíº Career/Experience Questions:
‚Ä¢ Highlight progression and growth
‚Ä¢ Mention leadership and mentoring experience
‚Ä¢ Include team sizes and methodologies
‚Ä¢ Example: "Over his 11+ years, Ramji has grown from Developer to Technical Lead..."

üöÄ Project Questions:
‚Ä¢ Describe business impact and scale
‚Ä¢ Mention technologies and architecture
‚Ä¢ Share metrics (data volume, users, performance gains)
‚Ä¢ Example: "In the Databridge project, Ramji built a system processing 100TB+ daily..."

üèÜ Skills/Expertise Questions:
‚Ä¢ Categorize by domain (Backend, Cloud, Big Data, etc.)
‚Ä¢ Mention proficiency level and years of experience
‚Ä¢ Give real project examples
‚Ä¢ Example: "Ramji's cloud expertise centers on AWS and Azure, where he's..."

üí¨ Contact/Hiring Questions:
‚Ä¢ Encourage using the contact form on this website
‚Ä¢ Mention LinkedIn for professional networking: ${LINKEDIN_PROFILE}
‚Ä¢ Note his location (Coimbatore, India) and current role
‚Ä¢ Example: "Interested in connecting with Ramji? The easiest way is..."

‚ùì Unknown/Unclear Questions:
‚Ä¢ Be honest about limitations
‚Ä¢ Suggest checking his LinkedIn profile or contact form
‚Ä¢ Offer to answer related questions you DO know
‚Ä¢ Example: "I don't have that specific information, but I can tell you about..."

üåü CONVERSATION STARTERS (if user is vague):
‚Ä¢ "Are you interested in Ramji's technical skills, project experience, or career journey?"
‚Ä¢ "I can tell you about his work in AdTech, Big Data, IoT, or Cloud architecture - what interests you?"
‚Ä¢ "Would you like to know about specific technologies he works with, or his leadership experience?"

LINKEDIN & CONTACT ROUTING:
‚Ä¢ For detailed resume/CV ‚Üí "Check his LinkedIn: ${LINKEDIN_PROFILE}"
‚Ä¢ For professional networking ‚Üí "Connect on LinkedIn: ${LINKEDIN_PROFILE}"
‚Ä¢ For job opportunities ‚Üí "Use the contact form below or reach out via LinkedIn"
‚Ä¢ For endorsements/recommendations ‚Üí "Visit his LinkedIn profile"
‚Ä¢ For technical discussions ‚Üí "Feel free to ask me here, or reach out through the contact form"

KEEP IT CONVERSATIONAL:
‚Ä¢ Ask follow-up questions when appropriate
‚Ä¢ Reference previous messages in the conversation
‚Ä¢ Use transitions like "Building on that..." or "Speaking of..."
‚Ä¢ End with engagement: "Would you like to know more about..." or "Any other questions about..."

Remember: You're Ramji's helpful AI assistant, not Ramji himself. Represent him professionally while being approachable and informative!`;

// Primary AI function using Groq (free tier: 14,400 requests/day, super fast!)
async function callGroqAPI(messages) {
  console.log('[GROQ] üöÄ Attempting Groq API call...');
  console.log('[GROQ] API Key present:', !!GROQ_API_KEY);
  console.log('[GROQ] API Key length:', GROQ_API_KEY?.length || 0);

  try {
    // Groq uses OpenAI-compatible API format - super easy!
    const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${GROQ_API_KEY}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'llama-3.3-70b-versatile', // Latest Groq model (Nov 2024), high quality and fast
        messages: messages,
        temperature: 0.7,
        max_tokens: 300
      })
    });

    console.log('[GROQ] Response status:', response.status, response.statusText);

    if (!response.ok) {
      const errorText = await response.text();
      console.error('[GROQ] ‚ùå API Error Response:', errorText);
      throw new Error(`Groq API failed: ${response.status} - ${errorText}`);
    }

    const data = await response.json();
    console.log('[GROQ] ‚úÖ Response received');
    console.log('[GROQ] Tokens used:', data.usage?.total_tokens || 0);

    const result = data.choices[0].message.content;
    console.log('[GROQ] ‚úÖ Reply length:', result.length);

    return result;
  } catch (error) {
    console.error('[GROQ] ‚ùå Exception:', error.message);
    throw error;
  }
}

// Fallback AI function using OpenAI (backup when Groq fails)
async function callOpenAI(messages) {
  console.log('[OPENAI] üîÑ Attempting OpenAI API call as fallback...');
  console.log('[OPENAI] API Key present:', !!OPENAI_API_KEY);

  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: messages,
        temperature: 0.7,
        max_tokens: 300,
        presence_penalty: 0.6,
        frequency_penalty: 0.3
      })
    });

    console.log('[OPENAI] Response status:', response.status, response.statusText);

    if (!response.ok) {
      const errorText = await response.text();
      console.error('[OPENAI] ‚ùå API Error Response:', errorText);
      throw new Error(`OpenAI API failed: ${response.status} - ${errorText}`);
    }

    const data = await response.json();
    console.log('[OPENAI] ‚úÖ Response received');
    console.log('[OPENAI] Tokens used:', data.usage?.total_tokens || 0);

    return {
      reply: data.choices[0].message.content,
      tokensUsed: data.usage?.total_tokens || 0
    };
  } catch (error) {
    console.error('[OPENAI] ‚ùå Exception:', error.message);
    throw error;
  }
}

export default async function handler(req, res) {
  // Log incoming request
  console.log(`[API] Incoming ${req.method} request from origin:`, req.headers.origin || 'unknown');

  // Enable CORS for all origins
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization, X-Requested-With, Accept, Origin');
  res.setHeader('Access-Control-Max-Age', '86400'); // 24 hours
  res.setHeader('Access-Control-Allow-Credentials', 'true');

  // Handle preflight OPTIONS request
  if (req.method === 'OPTIONS') {
    console.log('[API] ‚úÖ Handling OPTIONS preflight request - CORS headers set');
    return res.status(200).end();
  }

  // Only allow POST requests for actual API calls
  if (req.method !== 'POST') {
    console.log('[API] ‚ùå Method not allowed:', req.method);
    return res.status(405).json({ error: 'Method not allowed. Use POST.' });
  }

  console.log('[API] ‚úÖ Processing POST request');

  try {
    const { message, conversationHistory = [] } = req.body;

    // Validate input
    if (!message || message.trim().length === 0) {
      return res.status(400).json({ error: 'Message is required' });
    }

    // Limit message length
    if (message.length > 500) {
      return res.status(400).json({ error: 'Message too long. Max 500 characters.' });
    }

    // Check if at least one AI provider is configured
    if (!GROQ_API_KEY && !OPENAI_API_KEY) {
      console.error('[API] ‚ùå No AI providers configured (need GROQ_API_KEY or OPENAI_API_KEY)');
      return res.status(500).json({
        error: 'Service temporarily unavailable',
        fallback: true,
        reply: "I'm currently in offline mode. Please try again later or use the contact form below."
      });
    }

    console.log('[API] ‚úÖ AI providers available:', {
      groq: !!GROQ_API_KEY,
      openai: !!OPENAI_API_KEY
    });

    // Build conversation messages
    const messages = [
      {
        role: 'system',
        content: SYSTEM_PROMPT
      },
      // Include last 4 messages from history for context
      ...conversationHistory.slice(-4),
      {
        role: 'user',
        content: message.trim()
      }
    ];

    console.log('[API] üéØ Starting AI request cascade: Groq ‚Üí OpenAI ‚Üí Fallback');

    let reply;
    let tokensUsed = 0;
    let provider = 'Unknown';

    // PRIORITY 1: Try Groq first (FREE, fast, 14,400 requests/day)
    if (GROQ_API_KEY) {
      try {
        console.log('[API] üöÄ Attempting Groq (Primary)...');
        reply = await callGroqAPI(messages);
        provider = 'Groq';
        console.log('[API] ‚úÖ Groq success!');
      } catch (groqError) {
        console.error('[API] ‚ùå Groq failed:', groqError.message);

        // PRIORITY 2: Try OpenAI as fallback
        if (OPENAI_API_KEY) {
          try {
            console.log('[API] üîÑ Groq failed, trying OpenAI (Secondary)...');
            const openaiResult = await callOpenAI(messages);
            reply = openaiResult.reply;
            tokensUsed = openaiResult.tokensUsed;
            provider = 'OpenAI';
            console.log('[API] ‚úÖ OpenAI fallback success!');
          } catch (openaiError) {
            console.error('[API] ‚ùå OpenAI also failed:', openaiError.message);
            // Will use local fallback
          }
        } else {
          console.log('[API] ‚ö†Ô∏è No OpenAI key available for fallback');
        }
      }
    }
    // If no Groq key, try OpenAI directly
    else if (OPENAI_API_KEY) {
      try {
        console.log('[API] üîÑ No Groq key, trying OpenAI directly...');
        const openaiResult = await callOpenAI(messages);
        reply = openaiResult.reply;
        tokensUsed = openaiResult.tokensUsed;
        provider = 'OpenAI';
        console.log('[API] ‚úÖ OpenAI success!');
      } catch (openaiError) {
        console.error('[API] ‚ùå OpenAI failed:', openaiError.message);
        // Will use local fallback
      }
    }

    // If we got a response, return it
    if (reply) {
      return res.status(200).json({
        reply: reply,
        tokensUsed: tokensUsed,
        provider: provider
      });
    }

    // PRIORITY 3: All AI providers failed, throw error to trigger fallback
    console.log('[API] ‚ùå All AI providers failed, returning fallback response');
    throw new Error('All AI providers unavailable');

  } catch (error) {
    console.error('Chat API Error:', error);

    // Return fallback response
    return res.status(500).json({
      error: 'Failed to generate response',
      message: error.message,
      fallback: true,
      reply: "I'm having trouble connecting right now. Please try again in a moment, or use the contact form below to reach Ramji directly."
    });
  }
}

